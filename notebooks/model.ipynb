{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import skimage\n",
    "\n",
    "from src.data_utils import Dataset\n",
    "from src.network import unet_3d\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "- load train and test datasets, create TensorFlow Datasets from them\n",
    "- define main architecutre params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.load_dataset(\n",
    "    '../data/processed/train_dataset.pckl'\n",
    ").create_tf_dataset().shuffle(50).repeat().batch(1)\n",
    "test_dataset = Dataset.load_dataset(\n",
    "    '../data/processed/test_dataset.pckl'\n",
    ").create_tf_dataset().shuffle(50).repeat().batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import skimage\n",
    "\n",
    "from src.data_utils import Dataset\n",
    "from src.network import unet_3d\n",
    "\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "train_dataset = Dataset.load_dataset(\n",
    "    '../data/processed/train_dataset.pckl'\n",
    ").create_tf_dataset().shuffle(50).repeat().batch(1)\n",
    "test_dataset = Dataset.load_dataset(\n",
    "    '../data/processed/test_dataset.pckl'\n",
    ").create_tf_dataset().shuffle(50).repeat().batch(1)\n",
    "\n",
    "iterator = tf.data.Iterator.from_structure(\n",
    "    train_dataset.output_types,\n",
    "    train_dataset.output_shapes\n",
    ")\n",
    "training_init_op = iterator.make_initializer(train_dataset)\n",
    "test_init_op = iterator.make_initializer(test_dataset)\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.0381794])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1: (?, ?, 128, 128, 16)\n",
      "conv2: (?, ?, 128, 128, 32)\n",
      "maxpool layer: (?, ?, 64, 64, 32)\n",
      "conv1: (?, ?, 64, 64, 32)\n",
      "conv2: (?, ?, 64, 64, 64)\n",
      "maxpool layer: (?, ?, 32, 32, 64)\n",
      "conv1: (?, ?, 32, 32, 64)\n",
      "conv2: (?, ?, 32, 32, 128)\n",
      "upconv layer: (?, ?, 64, 64, 128)\n",
      "concat layer: (?, ?, 64, 64, 64)\n",
      "up_conv layer1: (?, ?, 64, 64, 64)\n",
      "up_conv layer2: (?, ?, 64, 64, 64)\n",
      "upconv layer: (?, ?, 128, 128, 64)\n",
      "concat layer: (?, ?, 128, 128, 32)\n",
      "up_conv layer1: (?, ?, 128, 128, 32)\n",
      "up_conv layer2: (?, ?, 128, 128, 32)\n",
      "output layer: (?, ?, 128, 128, 3)\n",
      "Epoch: 0, loss: 0.168, training accuracy: 42.98%\n",
      "Epoch: 0, loss: 0.168, training IOU: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# create the neural network model\n",
    "logits = unet_3d(next_element[0], depth=3)\n",
    "\n",
    "# add the optimizer and weigthed softmax loss\n",
    "# see https://stackoverflow.com/a/44563055\n",
    "class_weights = tf.cast(tf.constant([[0.00408978, 0.70013423, 0.295776]]), tf.float32)\n",
    "# deduce weights for batch samples based on their true label\n",
    "class_weights = tf.reduce_sum(tf.cast(next_element[0], tf.float32) * class_weights, axis=-1)\n",
    "loss = tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=next_element[1], weights=class_weights)\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# get accuracy\n",
    "prediction = tf.argmax(logits, axis=-1)\n",
    "labels = tf.argmax(next_element[1], -1)\n",
    "equality = tf.equal(prediction, labels)\n",
    "accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
    "\n",
    "# get mean iou\n",
    "\n",
    "\n",
    "# run the training\n",
    "epochs = 20\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(training_init_op)\n",
    "    for i in range(epochs):\n",
    "        iou, conf_mat = tf.metrics.mean_iou(labels, tf.cast(prediction, tf.int32), 3)\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        l, _, acc, miou = sess.run([loss, optimizer, accuracy, iou])\n",
    "        if i % 10 == 0:\n",
    "            print(\"Epoch: {}, loss: {:.3f}, training accuracy: {:.2f}%\".format(i, l, acc * 100))\n",
    "            print(\"Epoch: {}, loss: {:.3f}, training IOU: {:.2f}%\".format(i, l, miou * 100))\n",
    "    # now setup the validation run\n",
    "    valid_iters = 10\n",
    "    # re-initialize the iterator, but this time with validation data\n",
    "    sess.run(test_init_op)\n",
    "    avg_acc = 0\n",
    "    for i in range(valid_iters):\n",
    "        acc = sess.run([accuracy])\n",
    "        avg_acc += acc[0]\n",
    "    print(\"Average validation set accuracy over {} iterations is {:.2f}%\".format(valid_iters, (avg_acc / valid_iters) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
