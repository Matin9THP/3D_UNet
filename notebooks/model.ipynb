{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import skimage\n",
    "\n",
    "from src.data_utils import Dataset\n",
    "from src.network import unet_3d\n",
    "\n",
    "# tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "- load train and test datasets, create TensorFlow Datasets from them\n",
    "- define main architecutre params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.load_dataset(\n",
    "    '../data/processed/train_dataset.pckl'\n",
    ").create_tf_dataset().shuffle(50).repeat().batch(1)\n",
    "test_dataset = Dataset.load_dataset(\n",
    "    '../data/processed/test_dataset.pckl'\n",
    ").create_tf_dataset().shuffle(50).repeat().batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = [p.scans for p in train_dataset.patients.values()]\n",
    "# y_train = [p.seg for p in train_dataset.patients.values()]\n",
    "# x_test = [p.scans for p in test_dataset.patients.values()]\n",
    "# y_test = [p.seg for p in test_dataset.patients.values()]\n",
    "\n",
    "# i = 28\n",
    "# inputs = x_train[i]\n",
    "# outputs = y_train[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = tf.data.Iterator.from_structure(\n",
    "    train_dataset.output_types,\n",
    "    train_dataset.output_shapes\n",
    ")\n",
    "training_init_op = iterator.make_initializer(train_dataset)\n",
    "test_init_op = iterator.make_initializer(test_dataset)\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1: (?, 32, 128, 128, 16)\n",
      "conv2: (?, 32, 128, 128, 32)\n",
      "maxpool layer: (?, 16, 64, 64, 32)\n",
      "conv1: (?, 16, 64, 64, 32)\n",
      "conv2: (?, 16, 64, 64, 64)\n",
      "maxpool layer: (?, 8, 32, 32, 64)\n",
      "conv1: (?, 8, 32, 32, 64)\n",
      "conv2: (?, 8, 32, 32, 128)\n",
      "maxpool layer: (?, 4, 16, 16, 128)\n",
      "conv1: (?, 4, 16, 16, 128)\n",
      "conv2: (?, 4, 16, 16, 256)\n",
      "upconv layer: (?, 8, 32, 32, 256)\n",
      "concat layer: (?, 8, 32, 32, 128)\n",
      "up_conv layer1: (?, 8, 32, 32, 128)\n",
      "up_conv layer2: (?, 8, 32, 32, 128)\n",
      "upconv layer: (?, 16, 64, 64, 128)\n",
      "concat layer: (?, 16, 64, 64, 64)\n",
      "up_conv layer1: (?, 16, 64, 64, 64)\n",
      "up_conv layer2: (?, 16, 64, 64, 64)\n",
      "upconv layer: (?, 32, 128, 128, 64)\n",
      "concat layer: (?, 32, 128, 128, 32)\n",
      "up_conv layer1: (?, 32, 128, 128, 32)\n",
      "up_conv layer2: (?, 32, 128, 128, 32)\n",
      "output layer: (?, 32, 128, 128, 3)\n",
      "Epoch: 0, loss: 586579.000, training accuracy: 44.97%\n",
      "Epoch: 0, loss: 586579.000, training IOU: 0.00%\n",
      "Epoch: 50, loss: 88810.438, training accuracy: 98.76%\n",
      "Epoch: 50, loss: 88810.438, training IOU: 0.00%\n",
      "Epoch: 100, loss: 69792.766, training accuracy: 97.76%\n",
      "Epoch: 100, loss: 69792.766, training IOU: 0.00%\n",
      "Epoch: 150, loss: 36057.117, training accuracy: 99.00%\n",
      "Epoch: 150, loss: 36057.117, training IOU: 0.00%\n",
      "Epoch: 200, loss: 26472.887, training accuracy: 99.08%\n",
      "Epoch: 200, loss: 26472.887, training IOU: 0.00%\n",
      "Epoch: 250, loss: 40075.559, training accuracy: 97.59%\n",
      "Epoch: 250, loss: 40075.559, training IOU: 0.00%\n",
      "Epoch: 300, loss: 28638.062, training accuracy: 98.44%\n",
      "Epoch: 300, loss: 28638.062, training IOU: 0.00%\n",
      "Epoch: 350, loss: 23391.441, training accuracy: 98.64%\n",
      "Epoch: 350, loss: 23391.441, training IOU: 0.00%\n",
      "Epoch: 400, loss: 17379.789, training accuracy: 99.03%\n",
      "Epoch: 400, loss: 17379.789, training IOU: 0.00%\n",
      "Epoch: 450, loss: 18998.510, training accuracy: 98.71%\n",
      "Epoch: 450, loss: 18998.510, training IOU: 0.00%\n",
      "Epoch: 500, loss: 17214.402, training accuracy: 98.88%\n",
      "Epoch: 500, loss: 17214.402, training IOU: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# create the neural network model\n",
    "logits = unet_3d(next_element[0])\n",
    "\n",
    "# add the optimizer and loss\n",
    "loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels=next_element[1], logits=logits))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# get accuracy\n",
    "prediction = tf.argmax(logits, axis=-1)\n",
    "labels = tf.argmax(next_element[1], -1)\n",
    "equality = tf.equal(prediction, labels)\n",
    "accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
    "\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# run the training\n",
    "epochs = 600\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    sess.run(training_init_op)\n",
    "    iou, conf_mat = tf.metrics.mean_iou(labels, tf.cast(prediction, tf.int32), 3)\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    for i in range(epochs):\n",
    "        l, _, acc, miou = sess.run([loss, optimizer, accuracy, iou])\n",
    "        if i % 50 == 0:\n",
    "            print(\"Epoch: {}, loss: {:.3f}, training accuracy: {:.2f}%\".format(i, l, acc * 100))\n",
    "            print(\"Epoch: {}, loss: {:.3f}, training IOU: {:.2f}%\".format(i, l, miou * 100))\n",
    "    # now setup the validation run\n",
    "    valid_iters = 100\n",
    "    # re-initialize the iterator, but this time with validation data\n",
    "    sess.run(validation_init_op)\n",
    "    avg_acc = 0\n",
    "    for i in range(valid_iters):\n",
    "        acc = sess.run([accuracy])\n",
    "        avg_acc += acc[0]\n",
    "    print(\"Average validation set accuracy over {} iterations is {:.2f}%\".format(valid_iters))                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "conv1: (1, 32, 128, 128, 16)\n",
      "conv2: (1, 32, 128, 128, 32)\n",
      "maxpool layer: (1, 16, 64, 64, 32)\n",
      "conv1: (1, 16, 64, 64, 32)\n",
      "conv2: (1, 16, 64, 64, 64)\n",
      "maxpool layer: (1, 8, 32, 32, 64)\n",
      "conv1: (1, 8, 32, 32, 64)\n",
      "conv2: (1, 8, 32, 32, 128)\n",
      "maxpool layer: (1, 4, 16, 16, 128)\n",
      "conv1: (1, 4, 16, 16, 128)\n",
      "conv2: (1, 4, 16, 16, 256)\n",
      "upconv layer: (1, 8, 32, 32, 256)\n",
      "concat layer: (1, 8, 32, 32, 128)\n",
      "up_conv layer1: (1, 8, 32, 32, 128)\n",
      "up_conv layer2: (1, 8, 32, 32, 128)\n",
      "upconv layer: (1, 16, 64, 64, 128)\n",
      "concat layer: (1, 16, 64, 64, 64)\n",
      "up_conv layer1: (1, 16, 64, 64, 64)\n",
      "up_conv layer2: (1, 16, 64, 64, 64)\n",
      "upconv layer: (1, 32, 128, 128, 64)\n",
      "concat layer: (1, 32, 128, 128, 32)\n",
      "up_conv layer1: (1, 32, 128, 128, 32)\n",
      "up_conv layer2: (1, 32, 128, 128, 32)\n",
      "output layer: (1, 32, 128, 128, 3)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3bbec3ffa6de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Make a training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--------------------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Save the training accuracy on the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3bbec3ffa6de>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(loss, net, opt, x, y)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mupdate_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUPDATE_OPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_or_create_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Numpy array to keep track of the accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[1;32m    475\u001b[0m       \u001b[0;31m# to be executed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m         output_gradients=output_gradients)\n\u001b[0m\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       output_gradients)\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_MaxPool3DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    180\u001b[0m   return gen_nn_ops.max_pool3d_grad(\n\u001b[1;32m    181\u001b[0m       \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m       \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m       \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m       \u001b[0mksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ksize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.load_dataset(\n",
    "    '../data/processed/train_dataset.pckl'\n",
    ").create_tf_dataset()\n",
    "\n",
    "# Define network and optimizer\n",
    "net = unet_3d\n",
    "opt = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "writer = tf.contrib.summary.create_file_writer('tmp')\n",
    "\n",
    "def loss(net, inputs, labels):\n",
    "    return tf.reduce_sum(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            logits=net(inputs), labels=labels\n",
    "        )\n",
    "    )\n",
    "\n",
    "def train_step(loss, net, opt, x, y):\n",
    "    # Perform a single step of optimization\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        opt.minimize(lambda: loss(net, x, y), global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "# Numpy array to keep track of the accuracy\n",
    "acc_history = np.zeros(50)\n",
    "\n",
    "# with writer.as_default():\n",
    "#     with tf.contrib.summary.always_record_summaries():\n",
    "        \n",
    "# Loop over the epochs\n",
    "for epoch in range(50):\n",
    "\n",
    "    # Initialize the metric\n",
    "    accuracy = tfe.metrics.Accuracy()\n",
    "\n",
    "    # For each epoch we shuffle the dataset\n",
    "    for (xb, yb) in tfe.Iterator(train_dataset.shuffle(40).batch(1)):\n",
    "        print('============================================')\n",
    "        # Save the loss on disk\n",
    "        # tf.contrib.summary.scalar('loss_value', loss(net, xb,yb))\n",
    "\n",
    "        # Make a training step\n",
    "        \n",
    "        train_step(loss, net, opt, xb, yb)\n",
    "        print('--------------------------------------------')\n",
    "        # Save the training accuracy on the batch\n",
    "        accuracy(tf.argmax(net(tf.constant(xb)), axis=1), tf.argmax(tf.constant(yb), axis=1))\n",
    "\n",
    "    # Save the overall accuracy in our vector\n",
    "    acc_history[epoch] = accuracy.result().numpy()\n",
    "            \n",
    "\n",
    "# At the end, plot the evolution of the training accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(acc_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(24), Dimension(128), Dimension(128), Dimension(3)])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(24), Dimension(128), Dimension(128), Dimension(3), Dimension(3)])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = tf.constant([[0.1, .45, .45]])\n",
    "w = tf.gather(class_weights, outputs)\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = tf.constant([[0.1, .45, .45]])\n",
    "class_weights = tf.broadcast_to(class_weights, outputs.shape.as_list())\n",
    "w = tf.multiply(tf.cast(outputs, class_weights.dtype), class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unweighted_losses = tf.nn.softmax_cross_entropy_with_logits_v2(labels=outputs, logits=y_pred[0,:,:,:,:])\n",
    "weighted_losses = unweighted_losses * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(24), Dimension(128), Dimension(128)])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unweighted_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "weights can not be broadcast to values. values.rank=3. weights.rank=4. values.shape=(24, 128, 128). weights.shape=(24, 128, 128, 3).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-74144fdbfff5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/python/ops/losses/losses_impl.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy\u001b[0;34m(onehot_labels, logits, weights, label_smoothing, scope, loss_collection, reduction)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m     return compute_weighted_loss(\n\u001b[0;32m--> 808\u001b[0;31m         losses, weights, scope, loss_collection, reduction=reduction)\n\u001b[0m\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/python/ops/losses/losses_impl.py\u001b[0m in \u001b[0;36mcompute_weighted_loss\u001b[0;34m(losses, weights, scope, loss_collection, reduction)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     with ops.control_dependencies((\n\u001b[0;32m--> 209\u001b[0;31m         weights_broadcast_ops.assert_broadcastable(weights, losses),)):\n\u001b[0m\u001b[1;32m    210\u001b[0m       \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m       \u001b[0minput_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.5/site-packages/tensorflow/python/ops/weights_broadcast_ops.py\u001b[0m in \u001b[0;36massert_broadcastable\u001b[0;34m(weights, values)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \" values.shape=%s. weights.shape=%s.\" % (\n\u001b[1;32m    102\u001b[0m                 \u001b[0m_ASSERT_BROADCASTABLE_ERROR_PREFIX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues_rank_static\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 weights_rank_static, values.shape, weights.shape))\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mweights_shape_static\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mvalues_shape_static\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: weights can not be broadcast to values. values.rank=3. weights.rank=4. values.shape=(24, 128, 128). weights.shape=(24, 128, 128, 3)."
     ]
    }
   ],
   "source": [
    "\n",
    "tf.losses.softmax_cross_entropy(outputs, y_pred[0,:,:,:,:], weights=w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
